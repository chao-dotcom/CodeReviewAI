5. DPO Preference Optimization
5.1 Why DPO is Perfect for Code Review
Direct Preference Optimization (DPO) aligns models with human preferences without requiring explicit reward modeling. For code review, this is exceptional because:
●	Natural Preferences: Reviews inherently have quality rankings
●	No Reward Model: Simplifies training pipeline
●	Stable Training: More stable than PPO
●	Sample Efficiency: Works with fewer examples
●	Real Feedback: Can use actual reviewer preferences
5.2 Preference Data Generation
5.2.1 Automatic Preference Pairs
We'll generate preference pairs without human annotation:
Method 1: Multi-Agent Comparison
For each diff:
  1. Generate reviews from 3-5 different agents
  2. Use Critic Agent to rank reviews
  3. Create pairs: (diff, better_review, worse_review)
Method 2: GitHub Signals
Use implicit signals from real PRs:
  - Reviews that led to code changes (preferred)
  - Reviews that were acknowledged (preferred)
  - Reviews that were ignored (dispreferred)
Method 3: LLM-as-Judge
Use GPT-4 to rank reviews based on:
  - Actionability
  - Correctness
  - Tone
5.2.2 Preference Dataset Format
{
  "prompt": "Review this code diff: [DIFF]",
  "chosen": "Good review with specific suggestions...",
  "rejected": "Vague review without actionable feedback..."
}
5.3 DPO Training
5.3.1 DPO Algorithm
DPO optimizes the model to prefer chosen reviews over rejected ones:
Loss = -log(σ(β * (log π_θ(y_w|x) - log π_ref(y_w|x)
              - (log π_θ(y_l|x) - log π_ref(y_l|x)))))

Where:
  y_w = preferred review
  y_l = dispreferred review
  π_θ = policy being trained
  π_ref = reference policy (LoRA model)
  β = temperature parameter
5.3.2 Implementation
from trl import DPOTrainer

# Load LoRA model as reference
model = AutoModelForCausalLM.from_pretrained("./lora_checkpoint")

# Configure DPO
training_args = DPOConfig(
    beta=0.1,  # KL divergence coefficient
    learning_rate=5e-7,  # Lower than LoRA
    num_train_epochs=1,  # Usually 1 epoch sufficient
    per_device_train_batch_size=4
)

# Train with preference pairs
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=None,  # Will use model copy
    train_dataset=preference_dataset,
    tokenizer=tokenizer,
    args=training_args
)

dpo_trainer.train()
5.4 DPO Experiments
●	Beta Sweep: Test β = 0.01, 0.1, 0.5, 1.0
●	Data Size: How many preference pairs needed?
●	Preference Source: Auto-generated vs human-labeled
●	LoRA + DPO vs Base: Measure improvement
●	Win Rate: A/B test against base and LoRA-only models
5.5 Continuous Improvement Loop
The system can improve continuously:
1. Deploy model to production
2. Collect user feedback (thumbs up/down on reviews)
3. Generate new preference pairs from feedback
4. Retrain DPO with updated preferences
5. A/B test new model vs old model
6. Deploy if win rate > 55%
7. Repeat

6. RAG Implementation
6.1 Why RAG for Code Review
RAG enhances review quality by providing relevant context:
●	Repository Context: Understanding codebase structure
●	Style Guides: Enforcing project-specific conventions
●	Past Reviews: Learning from previous feedback
●	Documentation: Referencing API docs and comments
6.2 Knowledge Base Construction
6.2.1 Data Sources
Source	Content	Update Frequency
Repository Code	Function definitions, docstrings	On every commit
Style Guides	CONTRIBUTING.md, style docs	On file change
Past Reviews	Merged PR comments	Daily batch
Documentation	README, API docs	On file change
6.2.2 Chunking Strategy
Code requires special chunking approaches:
●	Function-level chunks: Each function = one chunk
●	Class-level chunks: Include class definition + methods
●	File-level metadata: Store imports and high-level structure
●	Overlap: Include function signatures of callers/callees
# Example chunk
{
  "type": "function",
  "name": "authenticate_user",
  "file": "src/auth.py",
  "code": "def authenticate_user(username, password): ...",
  "docstring": "Authenticates user with credentials...",
  "imports": ["hashlib", "jwt"],
  "calls": ["hash_password", "verify_token"]
}
6.3 Embedding Generation
Use code-specific embedding models:
●	Model: CodeBERT or UniXcoder (125M parameters)
●	Embedding Dimension: 768
●	Fine-tuning (optional): Contrastive learning on code pairs
from transformers import AutoTokenizer, AutoModel

model = AutoModel.from_pretrained("microsoft/codebert-base")
tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")

def embed_code(code_text):
    inputs = tokenizer(code_text, return_tensors='pt', 
                       truncation=True, max_length=512)
    outputs = model(**inputs)
    # Use [CLS] token embedding
    return outputs.last_hidden_state[0, 0, :].detach().numpy()
6.4 Vector Database Setup
We'll use ChromaDB for simplicity:
import chromadb

# Initialize client
client = chromadb.PersistentClient(path='./chroma_db')

# Create collection
collection = client.create_collection(
    name='code_review_kb',
    metadata={'description': 'Repository context for RAG'}
)

# Add documents
collection.add(
    documents=[chunk['code'] for chunk in chunks],
    embeddings=[chunk['embedding'] for chunk in chunks],
    metadatas=[chunk['metadata'] for chunk in chunks],
    ids=[chunk['id'] for chunk in chunks]
)
6.5 Retrieval Strategy
Multi-stage retrieval for code review:
Stage 1: File-level retrieval
  - Query: Changed files in PR
  - Retrieve: Related files (imports, callees)

Stage 2: Function-level retrieval
  - Query: Changed functions
  - Retrieve: Similar functions, test cases

Stage 3: Style guide retrieval
  - Query: Language + detected pattern
  - Retrieve: Relevant style rules
6.6 Context Assembly
Assemble retrieved context into agent prompt:
def build_review_prompt(diff, retrieved_context):
    prompt = f'''
You are reviewing this code diff:

{diff}

Relevant repository context:
{retrieved_context['similar_functions']}

Project style guide:
{retrieved_context['style_rules']}

Past reviews of similar code:
{retrieved_context['past_reviews']}

Provide a thorough code review.
    '''
    return prompt

7. Web Frontend & UI
7.1 Technology Stack
●	Framework: React 18 + TypeScript
●	Styling: Tailwind CSS
●	State Management: Zustand or React Context
●	Code Highlighting: Prism.js or Monaco Editor
●	Diff Rendering: react-diff-viewer-continued
7.2 Core UI Components
7.2.1 PR Dashboard
Main landing page showing all PRs:
●	List view with filters:
○	Status (pending, reviewed, approved)
○	Repository
○	Author
○	Date range
●	PR cards showing:
○	Title and description
○	Review progress (% complete)
○	Agent findings count
○	Severity distribution (critical/high/medium/low)
7.2.2 Diff Viewer
Split-pane diff view with inline comments:
●	Features:
○	Syntax highlighting
○	Line-by-line comparison
○	Inline comment threads
○	Collapsible unchanged sections
○	File tree navigation
// Example component structure
const DiffViewer = () => {
  return (
    <div className='flex h-screen'>
      <FileTree files={changedFiles} />
      <SplitDiffView
        oldCode={oldVersion}
        newCode={newVersion}
        comments={agentComments}
      />
      <CommentPanel />
    </div>
  );
};
7.2.3 Agent Trace Panel
Unique feature showing agent decision process:
●	Timeline visualization of agent execution
●	Show which agents ran and in what order
●	Display intermediate outputs
●	Show retrieval context used
●	Token usage per agent
7.2.4 Review Summary
Aggregated view of all findings:
●	Grouped by severity
●	Grouped by agent
●	Grouped by file
●	Overall recommendation (approve/request changes/comment)
7.3 User Interactions
●	Add/Edit Comments: Inline on any line
●	Resolve Threads: Mark agent comments as resolved
●	Request Re-review: Trigger agents on specific files
●	Approve/Reject: Human final decision
●	Feedback: Thumbs up/down on individual comments (for DPO)
7.4 Responsive Design
Mobile-friendly considerations:
●	Collapsible sidebars
●	Vertical diff view on small screens
●	Bottom sheet for comments on mobile
●	Touch-friendly hit targets

8. Backend API & Infrastructure
8.1 API Architecture
RESTful API built with FastAPI:
# Core endpoints
POST   /api/reviews          # Create new review request
GET    /api/reviews/{id}     # Get review status
GET    /api/reviews/{id}/comments  # Get all comments
POST   /api/reviews/{id}/feedback  # Submit feedback

POST   /api/webhooks/github  # GitHub webhook handler
POST   /api/webhooks/gitlab  # GitLab webhook handler

GET    /api/agents           # List available agents
GET    /api/agents/{id}/trace  # Get agent execution trace
8.2 Request Flow
1. Webhook received (PR created/updated)
   ↓
2. Validate and parse payload
   ↓
3. Queue review job (Redis + Celery)
   ↓
4. Worker picks up job
   ↓
5. Agent orchestrator executes review
   ↓
6. Results saved to database
   ↓
7. Webhook sent back to GitHub/GitLab
   ↓
8. UI polls for updates (or WebSocket)
8.3 Database Schema
Table: reviews
  - id (UUID, PK)
  - pr_url (TEXT)
  - status (ENUM: pending, in_progress, completed)
  - created_at, updated_at (TIMESTAMP)
  - metadata (JSONB)

Table: comments
  - id (UUID, PK)
  - review_id (UUID, FK)
  - agent_id (TEXT)
  - file_path (TEXT)
  - line_number (INT)
  - severity (ENUM: critical, high, medium, low, info)
  - content (TEXT)
  - metadata (JSONB)

Table: feedback
  - id (UUID, PK)
  - comment_id (UUID, FK)
  - user_id (UUID)
  - rating (INT: -1, 0, 1)  # thumbs down, neutral, up
  - created_at (TIMESTAMP)
8.4 Model Serving
8.4.1 Inference Optimization
●	Model Loading:
○	Load base model once on server start
○	Load LoRA adapters dynamically
○	Use 8-bit quantization (bitsandbytes)
●	Batching:
○	Batch multiple agent calls together
○	Use dynamic batching (wait 50ms for batch to fill)
●	Caching:
○	Cache embeddings for unchanged files
○	Cache retrieval results for 1 hour
8.4.2 Resource Management
# GPU allocation
CUDA_VISIBLE_DEVICES=0  # Inference server

# Memory limits
Max context length: 4096 tokens
Batch size: 4

# Timeout
Max inference time: 30 seconds per agent
Total review timeout: 5 minutes
8.5 Authentication & Authorization
●	OAuth 2.0 with GitHub/GitLab
●	JWT tokens for API access
●	API keys for programmatic access
●	Rate limiting: 100 requests/hour per user
8.6 Monitoring & Logging
●	Application Metrics:
○	Request latency (p50, p95, p99)
○	Error rates
○	Review completion rate
●	Model Metrics:
○	Inference latency
○	Token throughput
○	GPU utilization
●	Business Metrics:
○	Reviews per day
○	User feedback scores
○	Comments accepted/rejected ratio

